# Data_engineering
## Обработка химических данных
____
### Цели проекта:
    1. Знакомство с структурой и синтаксисом Python. [x]
    2. Изучение механизма обработки данных. [x]
    3. Изучение Jupyter notebook, EDA, Markdown [x]
______
### Ключевые этапы:
    1. Создание репозитория проекта и поиск необходимого датасета. [x]
    
    2. Выгрузка данных в датасет с API. [x]

    3. Создание скрипта data_Loader.py для выгрузки из Google Drive. [x]
        Настройка venv и пакетного менеджера Conda.

    3. Приведение типов для выбранного датасета и сохранение его в формате .parquet. [x]

    4. Проведение EDA на датасете при помощи jupiter notebook. [x]

    5. Подключение к БД SQLite и запись 100 строк. [x]

    6. Визуализация данных Seaborn. [x]

    7. Сбор пакета etl. [x]
_____
## Создание репозитория проекта и поиск необходимого датасета

* В качестве программы для работы с проектом был выбран **Pycharm**. 

* Было настроено виртуальное окружение с помощью комманд:

`pip install`

* В качестве данных было выбрано использовать химические данные при использовании библиотеки **PubChem** из за области специализации исследователя и личного интереса.

* Для работы с проектом необходимо создать виртуальное окружение на базе **pip**, а затем воспользоваться командой 

`pip install -r requirements.txt`
___________

## Выгрузка данных в датасет с API
* Получение данных происходит с **PubChem** при помощи API. Для этого используется скрипт `collect.py`.
Скрипт запрашивает следующие данные: 

>"MolecularFormula", "MolecularWeight", "CanonicalSMILES",
    "IsomericSMILES", "InChIKey", "XLogP", "TPSA",
    "HBondDonorCount", "HBondAcceptorCount", "RotatableBondCount",
    "ExactMass", "MonoisotopicMass", "HeavyAtomCount",
    "Charge", "Complexity"

* Скрипт возращает около 2000 строк, которые затем преобразуются в csv файл. 

* "Сырой" файл расположен по [ссылке](https://drive.google.com/file/d/1ikuXF1TNjzX6-_GKWLaPh_9Jz0txgVDM/view?usp=drive_link).

* Описание собранных данных:

> 1. CID - номер соединения
> 2. MolecularFormula - Молекулярная формула 
> 3. MolecularWeight - Молекулярный вес
> 4. SMILES- Запись химических соединения в международной системе SMILES
> 5. ConnectivitySMILES - Запись химических соединений в международной системе SMILES 
> 6. InChIKey - Идентификационный ключ химического соединения, по которому его можно будет идентифицировать
> 7. ExactMass — точная масса молекулы  
> 8. MonoisotopicMass — масса с учётом наиболее распространённых изотопов  
> 9. TPSA — полярная площадь поверхности  
> 10. Complexity — показатель сложности соединения
> 11. Charge — заряд молекулы
> 12. HBondDonorCount — количество доноров водородных связей  
> 13. HBondAcceptorCount — количество акцепторов водородных связей  
> 14. RotatableBondCount — число вращающихся связей  
> 15. HeavyAtomCount — количество тяжёлых атомов  
> 16. IsotopeAtomCount — количество атомов-изотопов
***

## Создание скрипта data_Loader.py для выгрузки из Google Drive. Настройка venv и пакетного менеджера Conda

* Создан файл data_loader. 
* Написан скрипт, позволяющий выгрузить данные с GoogleDrive. Скрипт преобразует данные в файл _csv_ для последующей удобной работы с ними. 
* Скриншот с результатом команды `raw_data.head(10)`

![Скриншот](screenshots/readme.png)
___
## Приведение типов для выбранного датасета и сохранение его в формате .parquet.
*  Файл data_loader дополняется приведением числовых и категориальных типов. 
* Далее датасет сохраняется в формате _parquet_. В дальнешем будет удобно использовать этот формат.
___

## Проведение EDA на датасете при помощи jupiter notebook.
* Для проведение EDA устанавливается jupiter notebook при помощи команды

`pip install jupiter notebook`

* Работа осуществляется на _html_ странице, которая вызывается в терминале при помощи команды

`jupiter notebook`

* Подробное описание работы с EDA описывается в файле EDA.ipynb или по этой [ссылке](https://nbviewer.org/github/mikhailovaelizabeth/Data_engineering/blob/main/notebooks/EDA.ipynb).


* Создание данной ссылки будет описано ниже, в пункте **Визуализация данных Seaborn**

___
## Подключение к БД SQLite и запись 100 строк
***Данный раздел не является одним из ключевых в рамках анализа данных и сделан для понимания работы с БД SQLite***

* Для подключения к базе данных скачивалось приложение DB Browser for SQlite.
Оно было необходимо для прочтеня файла формата .db


* Извлеченные данные сохранялись в файл, который предварительно был указан в *.gitignore*. Это имеет ! особую важность ! во избежание утечки информации.


* Был написан скрипт *write_to_db.py*, который:
  * считывал данные для доступа из *секретного* файла, 
  * создавал таблицу из 100 строк с данными из датасета
  * загружал их на сервер


* Для проверки корректности написанного кода было скачано приложение **DBeaver**. В нем проверялось наличие таблицы с данными. 

___

## Визуализация данных Seaborn
* Была создана визуализация для столбцов
    * MolecularWeight
    * MolecularWeight и соотношение с XLogP
    * HBondAcceptorCount и соотнощение с TPSA
    * А также темепературное распределение

* Дизайн графиков настраивался при помощи команды

`pip install seaborn`

при дальнейшем использовании комнад `CUSTOM_PALETTE` и  `sns.set_theme`

* Создание html ссылки для презентации EDA создавалось при помощи сайта **nbviewer**.
* Результат рендера ноутбука расположен по [ссылке](https://nbviewer.org/github/mikhailovaelizabeth/Data_engineering/blob/main/notebooks/EDA.ipynb)

___
## Сбор пакета etl

Структура проекта: 

    project-root/
    ├─ etl/
    │  ├─ __init__.py     
    │  ├─ extract.py    
    │  ├─ transform.py
    │  ├─ validate.py
    │  ├─ load.py
    │  ├─ collect.py
    │  └─ main.py
    ├─ data/
    │  ├─ raw/             
    │  └─ processed/      
    ├─ notebooks/
    │  └─ EDA.ipynb
    ├─ screenshots/
    ├─ README.md
    ├─ requirements.txt
    └─ .gitignore

Для работы с программой предусмотрены следующие комманды: 

1. `python -m etl.main --source-type collect `
    - Получение данных по API
    - Сохранение полученных данных в файл формата .csv
    - Создание файла формата .parquet на основе предыдущего .csv файла


2. `python -m etl.main --source-type gdrive`   
    - Работает при условии, что данные уже собраны и на данный момент находятся на google disk
    - Скачивание файла .csv с google disk
    - Запись файла .csv d .parquet


3. `python -m etl.main --source-type local`
   - Работает при условии, что необходимый файл уже находится в репозитории
   - Запись файла .csv d .parquet
   - Если необходимый файл не будет найдет, будет предложено в терминале выбрать сценарий сбора данных:
     

        0 — собрать данные с PubChem 
        1 — скачать данные с Google Drive
        2 — завершить программу <3

 
4. `python -m etl.main --source-type local --to-db`
   - Записывает из файла .parquet данные в базу данных (по умолчанию эта функция отключена)

   
5. `python -m etl.main --source-type local --max-rows`
    - При необходимости записать данные в базу данных и изменить колличество строк (По умолчанию - 100)


6.  `python -m etl.main --help`
    - Комманда подсказка - в терминале выводит доступные комманды

___

### Итоги 
В ходе работы над проектом были достигнуты все поставленные цели: 

    1. Освоены основы работы с языком Python и библиотеками, применяемыми для анализа данных.

    2. Получен практический опыт взаимодействия с PubChem API и обработки химических данных.

    3. Реализован полноценный ETL-пайплайн, включающий:

        сбор данных через API;
        загрузку файлов с Google Drive;
        преобразование и сохранение данных в формате .parquet;
        запись части данных в SQLite-базу;
        проведение EDA и визуализацию результатов при помощи Seaborn.

    4. Создана структурированная файловая архитектура проекта с возможностью гибкого выбора источника данных и дальнейшего расширения.

    5. Получен опыт работы с инструментами управления окружением (venv, conda, poetry) и системами контроля версий (GitHub).

#### Проект показал, как можно автоматизировать обработку химических данных и упростить дальнейший анализ, что может быть полезно для исследователей и специалистов в области химической технологии и data engineering.

**Спасибо за прочтение и внимание к проекту. Хорошего дня!**