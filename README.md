# Data_engineering
## Обработка химических данных
____
### Цели проекта:
    1. Знакомство с структурой и синтаксисом Python. [x]
    2. Изучение механизма обработки данных. [x]
    3. Изучение Jupyter notebook, EDA, Markdown [x]
______
### Ключевые этапы:
    1. Создание репозитория проекта и поиск необходимого датасета. [x]
    
    2. Выгрузка данных в датасет с API. [x]

    3. Создание скрипта data_Loader.py для выгрузки из Google Drive. [x]
        Настройка venv и пакетного менеджера Conda.

    3. Приведение типов для выбранного датасета и сохранение его в формате .parquet. [x]

    4. Проведение EDA на датасете при помощи jupiter notebook. [x]

    5. Подключение к БД SQLite и запись 100 строк. [x]

    6. Визуализация данных Seaborn. [x]

    7. Сбор пакета etl. [ ]

_____
## Создание репозитория проекта и поиск необходимого датасета

* В качестве программы для работы с проектом был выбран **Pycharm**. 
* Было настроено виртуальное окружение с помощью комманд:
`pip install`
* В качестве данных было выбрано использовать химические данные при использовании библиотеки **PubChem** из за области специализации исследователя и личного интереса.
___________

## Выгрузка данных в датасет с API
* Получение данных происходит с **PubChem** при помощи API. Для этого используется скрипт `collect.py`.
Скрипт запрашивает следующие данные: 

>"MolecularFormula", "MolecularWeight", "CanonicalSMILES",
    "IsomericSMILES", "InChIKey", "XLogP", "TPSA",
    "HBondDonorCount", "HBondAcceptorCount", "RotatableBondCount",
    "ExactMass", "MonoisotopicMass", "HeavyAtomCount",
    "Charge", "Complexity"

* Скрипт возращает около 2000 строк, которые затем преобразуются в csv файл. 

* "Сырой" файл расположен по [ссылке](https://drive.google.com/file/d/1ikuXF1TNjzX6-_GKWLaPh_9Jz0txgVDM/view?usp=drive_link).
***
Для работы с проектом необходимо создать виртуальное окружение на базе **pip**, а затем воспользоваться командой 

`pip install -r requirements.txt`
***

## Создание скрипта data_Loader.py для выгрузки из Google Drive. Настройка venv и пакетного менеджера Conda

* Для работы скрипта соездается переменное окружение с помощью команд: 

    `pip install conda`

    `pip install poetry`

* Создан файл data_loader. 
* Написан скрипт, позволяющий выгрузить данные с GoogleDrive. Скрипт преобразует данные в файл _csv_ для последующей удобной работы с ними. 
* Скриншот с результатом команды `raw_data.head(10)`

![Скриншот](screenshots/readme.png)
___
## Приведение типов для выбранного датасета и сохранение его в формате .parquet.
*  Файл data_loader дополняется приведением числовых и категориальных типов. 
* Далее датасет сохраняется в формате _parquet_. В дальнешем будет удобно использовать этот формат.
___

## Проведение EDA на датасете при помощи jupiter notebook.
* Для проведение EDA устанавливается jupiter notebook при помощи команды

`pip install jupiter notebook`

* Работа осуществляется на _html_ странице, которая вызывается в терминале при помощи команды

`jupiter notebook`

* Подробное описание работы с EDA описывается в файле EDA.ipynb или по этой [ссылке](https://nbviewer.org/github/mikhailovaelizabeth/Data_engineering/blob/main/notebooks/EDA.ipynb).


* Создание данной ссылки будет описано ниже, в пункте **Визуализация данных Seaborn**

___
## Подключение к БД SQLite и запись 100 строк
***Данный раздел не является одним из ключевых в рамках анализа данных и сделан для понимания работы с БД SQLite***

* Для подключения к базе данных скачивалось приложение DB Browser for SQlite.
Оно было необходимо для прочтеня файла формата .db


* Извлеченные данные сохранялись в файл, который предварительно был указан в *.gitignore*. Это имеет ! особую важность ! во избежание утечки информации.


* Был написан скрипт *write_to_db.py*, который:
  * считывал данные для доступа из *секретного* файла, 
  * создавал таблицу из 100 строк с данными из датасета
  * загружал их на сервер


* Для проверки корректности написанного кода было скачано приложение **DBeaver**. В нем проверялось наличие таблицы с данными. 

___

## Визуализация данных Seaborn
* Была создана визуализация для столбцов
    * MolecularWeight
    * MolecularWeight и соотношение с XLogP
    * HBondAcceptorCount и соотнощение с TPSA
    * А также темепературное распределение

* Дизайн графиков настраивался при помощи команды

`pip install seaborn`

при дальнейшем использовании комнад `CUSTOM_PALETTE` и  `sns.set_theme`

* Создание html ссылки для презентации EDA создавалось при помощи сайта **nbviewer**.
* Результат рендера ноутбука расположен по [ссылке](https://nbviewer.org/github/mikhailovaelizabeth/Data_engineering/blob/main/notebooks/EDA.ipynb)

___
## Сбор пакета etl
***в данный момент в разработке***

    project-root/
    ├─ etl/
    │  ├─ __init__.py
    │  ├─ extract.py
    │  ├─ transform.py
    │  ├─ validate.py
    │  ├─ load.py
    │  └─ main.py
    ├─ data/
    │  ├─ raw/             # сюда сохраняем исходники (.csv)
    │  └─ processed/       # сюда сохраняем .parquet/.csv
    ├─ README.md
    ├─ requirements.txt
    ├─ .gitignore
    └─ (удалённые) data_loader.py, write_to_db.py 

___

### Итоги 
    Были проведены 
    1. лол
    2. кек

Спасибо за прочтение и внимание к проекту! 